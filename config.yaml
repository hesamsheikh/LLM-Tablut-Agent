provider: remote  # 'ollama' or 'remote'
llm_color: "BLACK"  # 'BLACK' or 'WHITE'

ollama:
  model: "gemma3:4b"
  temperature: 0.2
  top_p: 0.3
  # ollama_host: "http://localhost:11434"  # Optional: Specify if Ollama runs elsewhere

remote:
  model: "deepseek-chat"
  temperature: 0.2
  top_p: 0.3
  remote_api_url: "https://api.deepseek.com/chat/completions"  # e.g., "https://api.openai.com/v1/chat/completions"
  remote_model_override: "deepseek-chat"  # Optional: Use if remote API model name differs from common 'model' 
  use_few_shot: true # Add few-shot examples to the system prompt

# configuration for player types and PPO settings
white_player: "random"  # Options: 'llm', 'llm_two_step', 'random'
black_player: "llm_two_step"  # Options: 'llm', 'llm_two_step', 'random'
# ppo_model_path: "model\\ppo_white_20250407_225550\\tablut_ppo_white_wr97_ep2700.pth" # Removed PPO
# ppo_temperature: 0.1 # Removed PPO
# ppo_use_cpu: false # Removed PPO

# Benchmark configuration
num_games: 12
eval_type: "few_shot" # Descriptive name for the benchmark run